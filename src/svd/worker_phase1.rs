use std::cmp;

use nalgebra as na;

use super::WorkerArgs;
use crate::common::{self, Float};

#[cfg_attr(feature = "profile", inline(never))]
#[cfg_attr(not(feature = "profile"), inline(always))]
pub(super) unsafe fn jts_group_worker_phase_1<D: na::Dim>(
    worker_id: usize,
    t: usize,
    args: &WorkerArgs<'_, D>,
) where
    na::DefaultAllocator: na::allocator::Allocator<Float, D, D>,
    na::DefaultAllocator: na::allocator::Allocator<Float, D>,
    na::DefaultAllocator: na::allocator::Allocator<(Float, usize), D>,
    na::DefaultAllocator: na::allocator::Allocator<(usize, usize), D>,
{
    let n = args.n;

    // The start index of the part of p to be written to by this thread, and the number of elements
    // to write
    let total_nouter_iters = n / 2;
    let ninner_iters = n + (n % 2) - 1;

    let (outer_iter_start_i, nouter_iters) =
        common::uneven_divide(worker_id, total_nouter_iters, t);

    let start_i = outer_iter_start_i * ninner_iters;
    let nelements = nouter_iters * ninner_iters;

    // ------------ PHASE 1.1 ------------
    // Generate all the possible column pairs (in parallel). Then sort each subset of column-pairs
    // generated by each thread (in parallel).
    {
        // SAFETY: all worker threads are the in phase 1.1. Here b is read-only, so casting the b
        // pointer to a MatrixSlice is ok.
        //
        // The pointer is also valid for n*n matrix (guaranteed by the caller)
        let b_slice = unsafe { std::slice::from_raw_parts(args.b.as_ptr(), n * n) };
        let b = na::MatrixSlice::from_slice_generic(b_slice, args.shape.0, args.shape.1);

        let mut i = start_i;

        for j1 in (worker_id..(n / 2)).step_by(t) {
            let k1 = (j1 + 1)..n;

            let j2 = n - j1 - 1 - (n % 2);
            let k2 = (j2 + 1)..n;

            for (j, k) in k1.map(|k| (j1, k)).chain(k2.map(|k| (j2, k))) {
                let d = b.column(j).dot(&b.column(k));

                // SAFETY: caller guarantees p1 points to a pre-allocated buffer which is writeable.
                // Each index here is unique and so writing to it in parallel is ok.
                unsafe { args.p1.as_ptr().add(i).write((j, k, d)) };

                i += 1;
            }
        }

        // SAFETY: caller guarantees p1 points to a pre-allocated buffer which is n choose 2 long.
        // The values of p1 being sorted here were written already by this thread above
        let p = unsafe { std::slice::from_raw_parts_mut(args.p1.as_ptr().add(start_i), nelements) };

        // Sort by descending order of dot-products
        p.sort_unstable_by(super::column_pair_cmp);
    }

    let mut sort_lock = args.sort_locks[worker_id]
        .lock()
        .expect("Couldn't lock self sort_lock");

    args.barrier.wait(); // Wait for all threads to fill p and to have their own locks

    // ------------ PHASE 1.2 ------------
    // Merge sorted subsets of column-pairs (in parallel). The main worker also sets a flag to
    // indicate completion.
    let nruns = (worker_id | t.next_power_of_two()).trailing_zeros();

    let mut p_cur = args.p1.as_ptr().add(start_i);
    let mut p_merge_to = args.p2.as_ptr().add(start_i);

    *sort_lock = nelements;

    for i in 0..nruns {
        let other_lock = match args.sort_locks.get(worker_id + 2usize.pow(i)) {
            Some(l) => l.lock().expect("couldn't lock other thread's sort_lock"),
            // If t is not a power of 2, then higher worker_ids may need to merge from threads
            // that don't exist. In that case, the thread can break early as the thread to
            // merge and reduce with is strictly increasing.
            None => break,
        };

        // SAFETY: We have already sorted the first *sort_lock number of elements from start_i and
        // the other thread did the same for *other_lock elements. There is also
        // *sort_lock + *other_lock elements available from start_i in p_merge_to.
        unsafe {
            let this_cur = std::slice::from_raw_parts(p_cur, *sort_lock);
            let other_cur = std::slice::from_raw_parts(p_cur.add(*sort_lock), *other_lock);
            merge(this_cur, other_cur, p_merge_to, super::column_pair_cmp);
        }

        std::mem::swap(&mut p_cur, &mut p_merge_to);

        *sort_lock += *other_lock;
    }

    drop(sort_lock);

    if worker_id == super::MAIN_WORKER {
        // Barrier takes care of synchronisation, so relaxed ordering is fine
        args.counter.store(
            // SAFETY: p has n choose 2 elements
            if unsafe { args.p.as_ptr().read().2 } < args.delta {
                super::COMPLETED
            } else {
                0
            },
            std::sync::atomic::Ordering::Relaxed,
        );
    }
}

/// # Safety
/// - to must have `a.len() + b.len()` available writing spots
unsafe fn merge<T: Copy + std::fmt::Debug, F: FnMut(&T, &T) -> cmp::Ordering>(
    a: &[T],
    b: &[T],
    to: *mut T,
    mut f: F,
) {
    let mut a_i = 0;
    let mut b_i = 0;
    let mut i = 0;

    while a_i < a.len() && b_i < b.len() {
        let ord = f(&a[a_i], &b[b_i]);
        if ord.is_lt() {
            to.add(i).write(a[a_i]);
            a_i += 1;
        } else {
            to.add(i).write(b[b_i]);
            b_i += 1;
        }
        i += 1;
    }

    while a_i < a.len() {
        to.add(i).write(a[a_i]);
        a_i += 1;
        i += 1;
    }

    while b_i < b.len() {
        to.add(i).write(b[b_i]);
        b_i += 1;
        i += 1;
    }
}
